{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "反向传播中的求导可以想象好多层神经网络的那个图\n",
    "\n",
    "最后一层是求了sum，只有一个神经元\n",
    "下面的那几层每一层有一些神经元，想象为一个向量\n",
    "每一次反向传播都是先分别对相邻的层数进行求导，第一层是标量对向量，第二层是向量对向量....\n",
    "结果全部乘起来（一堆的矩阵乘法），就是某一层参数的梯度（微积分里面学的复合函数求导法则）\n",
    "\n",
    "---\n",
    "\n",
    "另外，如果用激活函数，求导的时候会产生一些对角矩阵，使得一些神经元的参数值变0（减小噪音干扰）\n",
    "如果用了dropout，效果同上（原理稍有不同，这个是直接设置为0，更加暴力一点），如果用LX正则的话，会增大梯度，大的增的多，小的增的少\n",
    "\n",
    "Example：神经元个数为1,n1,n2... 最后矩阵的形状就是 1xn1xn1xn2.... 反正会是向量（相当巧妙，正好和当初设定的w形状相同） \n",
    "\n",
    "至于batch_size，虽然实际上它将以上所说的东西的维数全部+1，但是可以将其独立出来看，就相当于求了batch_size个导数，然后取个平均值使得改变参数的时候更加合理准确以免梯度过大或者过小导致误差变大                                                        24.12.3日记"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
